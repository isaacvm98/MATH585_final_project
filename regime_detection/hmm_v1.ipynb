{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n","from scipy.stats import multivariate_normal\n","from pandas.plotting import register_matplotlib_converters\n","register_matplotlib_converters()\n","\n","# Instantiate a QuantBook.\n","qb = QuantBook()\n","\n","# Select the desired tickers for research.\n","asset = \"SPX\"\n","\n","train_cutoff_date = datetime(2024,12, 31)\n","\n","# Call the AddIndex method with the tickers, and its corresponding resolution. Then store their Symbols. Resolution.Minute is used by default. \n","qb.add_index(asset, Resolution.MINUTE)\n","\n","# Call the history method with qb.securities.keys for all tickers, time argument(s), and resolution to request historical data for the symbol.\n","history = qb.history(qb.securities.keys(), datetime(2019, 1, 1), datetime(2025, 11, 10), Resolution.DAILY)\n","\n","# Get the close price daily return.\n","close = history['close'].unstack(level=0)\n","\n","# Call pct_change to obtain the daily return\n","returns = close.pct_change().iloc[1:]\n","\n","# Initialize the HMM, then fit by the daily return data. Note that we're using varinace as switching regime, so switching_variance is set as True.\n","model = MarkovRegression(returns, k_regimes=2, switching_variance=True).fit()\n","print(model.summary())"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Get the regime as a column, 1 as Low Variance Regime, 2 as High Variance Regime.\n","regime = pd.Series(model.smoothed_marginal_probabilities.values.argmax(axis=1)+1, \n","                      index=returns.index, name='regime')\n","df_1 = close.loc[returns.index][regime == 1]\n","df_2 = close.loc[returns.index][regime == 2]\n","\n","# Get the mean and covariance matrix of the 2 regimes, assume 0 covariance between the two.\n","mean = np.array([returns.loc[df_1.index].mean(), returns.loc[df_2.index].mean()])\n","cov = np.array([[returns.loc[df_1.index].var()[0], 0], [0, returns.loc[df_2.index].var()[0]]])\n","\n","# Fit a 2-dimensional multivariate normal distribution by the 2 means and covriance matrix.\n","dist = multivariate_normal(mean=mean.flatten(), cov=cov)\n","mean_1, mean_2 = mean[0], mean[1]\n","sigma_1, sigma_2 = cov[0,0], cov[1,1]\n","\n","\n","# Plot the probability of data in different regimes.\n","fig, axes = plt.subplots(2, figsize=(15, 10))\n","ax = axes[0]\n","ax.plot(model.smoothed_marginal_probabilities[0])\n","ax.set(title='Smoothed probability of Low Variance Regime')\n","ax = axes[1]\n","ax.plot(model.smoothed_marginal_probabilities[1])\n","ax.set(title='Smoothed probability of High Variance Regime')\n","fig.tight_layout()\n","plt.show()\n","\n","# Plot the series into regime-wise.\n","df_1.index = pd.to_datetime(df_1.index)\n","df_1 = df_1.sort_index()\n","df_2.index = pd.to_datetime(df_2.index)\n","df_2 = df_2.sort_index()\n","\n","plt.figure(figsize=(15, 10))\n","plt.scatter(df_1.index, df_1, color='blue', label=\"Low Variance Regime\")\n","plt.scatter(df_2.index, df_2, color='red', label=\"High Variance Regime\")\n","\n","# Add shaded region with label\n","plt.axvspan('2024-07-01', '2025-07-01', alpha=0.2, color='green', zorder=-1, label=\"Backtesting Period\")\n","\n","plt.title(\"SPY Price\")\n","plt.ylabel(\"Price ($)\")\n","plt.xlabel(\"Date\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# 1. Distribution of returns for each regime\n","fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n","\n","ax = axes[0]\n","returns_regime_1 = returns.loc[df_1.index]\n","ax.hist(returns_regime_1.values.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n","ax.axvline(mean_1[0], color='darkblue', linestyle='--', linewidth=2, label=f'Mean: {mean_1[0]:.4f}')\n","ax.set_title('Distribution of Returns - Low Variance Regime')\n","ax.set_xlabel('Returns')\n","ax.set_ylabel('Frequency')\n","ax.legend()\n","ax.grid(alpha=0.3)\n","\n","ax = axes[1]\n","returns_regime_2 = returns.loc[df_2.index]\n","ax.hist(returns_regime_2.values.flatten(), bins=50, alpha=0.7, color='red', edgecolor='black')\n","ax.axvline(mean_2[0], color='darkred', linestyle='--', linewidth=2, label=f'Mean: {mean_2[0]:.4f}')\n","ax.set_title('Distribution of Returns - High Variance Regime')\n","ax.set_xlabel('Returns')\n","ax.set_ylabel('Frequency')\n","ax.legend()\n","ax.grid(alpha=0.3)\n","\n","fig.tight_layout()\n","plt.show()\n","\n","plt.figure(figsize=(15, 6))\n","plt.hist(returns_regime_1.values.flatten(), bins=50, alpha=0.5, color='blue', \n","         label=f'Low Variance (σ²={sigma_1:.6f})', edgecolor='black')\n","plt.hist(returns_regime_2.values.flatten(), bins=50, alpha=0.5, color='red', \n","         label=f'High Variance (σ²={sigma_2:.6f})', edgecolor='black')\n","plt.axvline(mean_1[0], color='darkblue', linestyle='--', linewidth=2)\n","plt.axvline(mean_2[0], color='darkred', linestyle='--', linewidth=2)\n","plt.title('Comparison of Return Distributions by Regime')\n","plt.xlabel('Returns')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.grid(alpha=0.3)\n","plt.show()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# 2. Analyze regime switching duration and frequency\n","regime_changes = regime.diff().fillna(0)\n","switches = regime_changes[regime_changes != 0]\n","\n","# Calculate duration of each regime period\n","regime_periods = []\n","current_regime = regime.iloc[0]\n","start_date = regime.index[0]\n","duration = 1\n","\n","for i in range(1, len(regime)):\n","    if regime.iloc[i] == current_regime:\n","        duration += 1\n","    else:\n","        # Regime changed, store the previous period\n","        regime_periods.append({\n","            'regime': current_regime,\n","            'start_date': start_date,\n","            'end_date': regime.index[i-1],\n","            'duration_days': duration\n","        })\n","        # Start new period\n","        current_regime = regime.iloc[i]\n","        start_date = regime.index[i]\n","        duration = 1\n","\n","# Add the last period\n","regime_periods.append({\n","    'regime': current_regime,\n","    'start_date': start_date,\n","    'end_date': regime.index[-1],\n","    'duration_days': duration\n","})\n","\n","regime_durations_df = pd.DataFrame(regime_periods)\n","\n","# Summary statistics\n","print(f\"\\n{'='*60}\")\n","print(\"REGIME SWITCHING ANALYSIS\")\n","print(f\"{'='*60}\")\n","print(f\"\\nTotal number of regime switches: {len(switches)}\")\n","print(f\"Total periods analyzed: {len(regime_durations_df)}\")\n","print(f\"\\nRegime 1 (Low Variance):\")\n","regime_1_durations = regime_durations_df[regime_durations_df['regime'] == 1]['duration_days']\n","print(f\"  Number of periods: {len(regime_1_durations)}\")\n","print(f\"  Average duration: {regime_1_durations.mean():.2f} days\")\n","print(f\"  Median duration: {regime_1_durations.median():.2f} days\")\n","print(f\"  Min duration: {regime_1_durations.min()} days\")\n","print(f\"  Max duration: {regime_1_durations.max()} days\")\n","\n","print(f\"\\nRegime 2 (High Variance):\")\n","regime_2_durations = regime_durations_df[regime_durations_df['regime'] == 2]['duration_days']\n","print(f\"  Number of periods: {len(regime_2_durations)}\")\n","print(f\"  Average duration: {regime_2_durations.mean():.2f} days\")\n","print(f\"  Median duration: {regime_2_durations.median():.2f} days\")\n","print(f\"  Min duration: {regime_2_durations.min()} days\")\n","print(f\"  Max duration: {regime_2_durations.max()} days\")\n","\n","fig, ax = plt.subplots(figsize=(8, 6))\n","\n","ax.boxplot([regime_1_durations, regime_2_durations], \n","           labels=['Low Variance', 'High Variance'],\n","           patch_artist=True,\n","           boxprops=dict(facecolor='lightblue', alpha=0.7),\n","           medianprops=dict(color='red', linewidth=2))\n","\n","ax.set_title('Regime Duration Comparison')\n","ax.set_ylabel('Duration (days)')\n","ax.grid(alpha=0.3)\n","\n","fig.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Calculate confidence metrics for regime predictions\n","regime_confidence = model.smoothed_marginal_probabilities.max(axis=1)\n","regime_uncertainty = 1 - regime_confidence  # or you could use entropy\n","\n","# Add confidence to our regime series\n","regime_with_confidence = pd.DataFrame({\n","    'regime': regime,\n","    'confidence': regime_confidence,\n","    'prob_regime_1': model.smoothed_marginal_probabilities[0],\n","    'prob_regime_2': model.smoothed_marginal_probabilities[1]\n","})\n","\n","print(f\"\\nOverall confidence statistics:\")\n","print(f\"  Mean confidence: {regime_confidence.mean():.4f}\")\n","print(f\"  Median confidence: {regime_confidence.median():.4f}\")\n","print(f\"  Min confidence: {regime_confidence.min():.4f}\")\n","print(f\"  Max confidence: {regime_confidence.max():.4f}\")\n","\n","# Identify low confidence periods (e.g., confidence < 0.7)\n","low_confidence_threshold = 0.7\n","low_confidence_periods = regime_with_confidence[regime_with_confidence['confidence'] < low_confidence_threshold]\n","print(f\"\\nPeriods with confidence < {low_confidence_threshold}: {len(low_confidence_periods)} days ({len(low_confidence_periods)/len(regime)*100:.2f}%)\")\n","\n","# Analyze confidence around regime switches\n","regime_changes_idx = regime_changes[regime_changes != 0].index\n","switch_window = 5  # days before and after switch\n","print(f\"\\n Confidence at switch statistics:\")\n","switch_confidence_analysis = []\n","for switch_date in regime_changes_idx:\n","    switch_loc = regime.index.get_loc(switch_date)\n","    start_idx = max(0, switch_loc - switch_window)\n","    end_idx = min(len(regime), switch_loc + switch_window + 1)\n","    \n","    window_dates = regime.index[start_idx:end_idx]\n","    window_confidence = regime_confidence.iloc[start_idx:end_idx]\n","    \n","    switch_confidence_analysis.append({\n","        'switch_date': switch_date,\n","        'from_regime': regime.iloc[switch_loc - 1] if switch_loc > 0 else None,\n","        'to_regime': regime.iloc[switch_loc],\n","        'confidence_at_switch': regime_confidence.iloc[switch_loc],\n","        'avg_confidence_before': window_confidence.iloc[:switch_window].mean() if switch_loc >= switch_window else None,\n","        'avg_confidence_after': window_confidence.iloc[switch_window:].mean() if switch_loc < len(regime) - switch_window else None\n","    })\n","\n","switch_confidence_df = pd.DataFrame(switch_confidence_analysis)\n","\n","print(f\"\\nAverage confidence at switch points: {switch_confidence_df['confidence_at_switch'].mean():.4f}\")\n","print(f\"\\nMedian confidence at switch points: {switch_confidence_df['confidence_at_switch'].median():.4f}\")\n","print(f\"Max confidence at switch points: {switch_confidence_df['confidence_at_switch'].max():.4f}\")\n","print(f\"Minimum confidence at switch points: {switch_confidence_df['confidence_at_switch'].min():.4f}\")\n","fig, ax = plt.subplots(figsize=(8, 6))\n","\n","ax.hist(switch_confidence_df['confidence_at_switch'], bins=20, \n","        color='purple', alpha=0.7, edgecolor='black')\n","ax.axvline(switch_confidence_df['confidence_at_switch'].mean(), \n","           color='red', linestyle='--', linewidth=2, \n","           label=f\"Mean: {switch_confidence_df['confidence_at_switch'].mean():.3f}\")\n","ax.axvline(switch_confidence_df['confidence_at_switch'].median(), \n","           color='orange', linestyle='--', linewidth=2, \n","           label=f\"Median: {switch_confidence_df['confidence_at_switch'].median():.3f}\")\n","ax.set_title('Distribution of Confidence at Regime Switches')\n","ax.set_xlabel('Confidence')\n","ax.set_ylabel('Number of Switches')\n","ax.legend()\n","ax.grid(alpha=0.3)\n","\n","fig.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from sklearn.covariance import EmpiricalCovariance\n","def calculate_turbulence(returns_df, lookback=252*3):\n","    \"\"\"Calculate Kritzman turbulence using Mahalanobis distance\"\"\"\n","    \n","    # Use sector returns only (not SPY)\n","    sector_returns = returns_df[[col for col in returns_df.columns if col != 'SPY']]\n","    \n","    turbulence = []\n","    \n","    for i in range(lookback, len(sector_returns)):\n","        # Historical window\n","        hist_data = sector_returns.iloc[i-lookback:i].values\n","        current_returns = sector_returns.iloc[i].values\n","        \n","        # Mean and covariance\n","        mu = hist_data.mean(axis=0)\n","        cov_estimator = EmpiricalCovariance()\n","        cov_estimator.fit(hist_data)\n","        cov = cov_estimator.covariance_\n","        \n","        # Mahalanobis distance\n","        diff = current_returns - mu\n","        try:\n","            turb_score = diff @ np.linalg.inv(cov) @ diff\n","        except:\n","            turb_score = diff @ np.linalg.pinv(cov) @ diff\n","        \n","        turbulence.append(turb_score)\n","    \n","    # Create series with NaN padding\n","    turbulence_series = pd.Series(\n","        [np.nan] * lookback + turbulence,\n","        index=sector_returns.index\n","    )\n","    \n","    return turbulence_series"]},{"cell_type":"code","execution_count":7,"id":"42c9d549","metadata":{},"outputs":[],"source":["sectors = ['XLK', 'XLF', 'XLE', 'XLV', 'XLY', 'XLP', 'XLI', 'XLU', 'XLB']\n","\n","for sector in sectors:\n","    qb.add_equity(sector)\n","\n","sectors_history = qb.history(sectors, datetime(2019, 1, 1), datetime(2025, 11, 10), Resolution.DAILY)\n","\n","# Get the close price daily return.\n","sectors_close = sectors_history['close'].unstack(level=0)\n","\n","# Call pct_change to obtain the daily return\n","sectors_returns = sectors_close.pct_change().iloc[1:]"]},{"cell_type":"code","execution_count":8,"id":"ae386a4d","metadata":{},"outputs":[],"source":["turbulence = calculate_turbulence(sectors_returns.dropna(), lookback=252*1)"]},{"cell_type":"code","execution_count":9,"id":"1a079bbe","metadata":{},"outputs":[],"source":["crisis_events = {\n","    'COVID-19': ('2020-02-20', '2020-04-01'),\n","    'Fed Tightening': ('2022-01-01', '2022-10-31'),\n","    'Tariff': ('2025-04-02', '2025-05-16')\n","}\n","\n","def evaluate_crisis_detection(states, crisis_events):\n","    \"\"\"Evaluate how well model detects known crises\"\"\"\n","    results = {}\n","    for event_name, (start, end) in crisis_events.items():\n","        mask = (states.index >= start) & (states.index <= end)\n","        if mask.sum() > 0:\n","            detection_rate = states[mask].mean() * 100\n","            results[event_name] = f\"{detection_rate:.1f}%\"\n","        else:\n","            results[event_name] = \"N/A\"\n","    return results\n","\n","def calculate_regime_metrics(states):\n","    \"\"\"Calculate stability metrics\"\"\"\n","    switches = (states.diff() != 0).sum()\n","    years = len(states) / 252\n","    \n","    return {\n","        'Switches/Year': f\"{switches/years:.1f}\",\n","        'Crisis %': f\"{states.mean()*100:.1f}%\",\n","        'Total Switches': switches\n","    }"]},{"cell_type":"code","execution_count":10,"id":"46adee21","metadata":{},"outputs":[],"source":["train_period = turbulence.iloc[252:252*4]  # Year 2-4 (after 1yr lookback)\n","threshold_95 = train_period.quantile(0.95)"]},{"cell_type":"code","execution_count":11,"id":"833a32fc","metadata":{},"outputs":[],"source":["turb_states = (turbulence > threshold_95).astype(int)\n","turb_states = turb_states.dropna()"]},{"cell_type":"code","execution_count":12,"id":"0905f033","metadata":{},"outputs":[],"source":["print(\"BASELINE: Kritzman Turbulence (95th Percentile Threshold)\")\n","print(\"=\"*70)\n","print(f\"Threshold: {threshold_95:.2f}\")\n","print(f\"\\nCrisis Detection:\")\n","turb_crisis = evaluate_crisis_detection(turb_states, crisis_events)\n","for event, rate in turb_crisis.items():\n","    print(f\"  {event}: {rate}\")\n","\n","print(f\"\\nRegime Stability:\")\n","turb_metrics = calculate_regime_metrics(turb_states)\n","for metric, value in turb_metrics.items():\n","    print(f\"  {metric}: {value}\")\n"]},{"cell_type":"code","execution_count":13,"id":"9df15ada","metadata":{},"outputs":[],"source":["def simple_volatility_baseline(returns, window=21, percentile=0.95):\n","    \"\"\"\n","    Simplest possible baseline: rolling volatility threshold\n","    \n","    Parameters:\n","    -----------\n","    returns : pd.Series\n","        Daily returns\n","    window : int\n","        Rolling window for volatility (21 = 1 month)\n","    percentile : float\n","        Threshold percentile\n","    \"\"\"\n","    # Calculate rolling volatility\n","    rolling_vol = returns.rolling(window=window).std() * np.sqrt(252)  # Annualized\n","    \n","    # Train threshold on first 3 years\n","    train_vol = rolling_vol.iloc[window:window+252*3]\n","    threshold = train_vol.quantile(percentile)\n","    \n","    # Classify regimes\n","    states = (rolling_vol > threshold).astype(int)\n","    \n","    return states.dropna(), rolling_vol.dropna(), threshold\n","\n","# Apply simple volatility baseline\n","vol_states, rolling_vol, vol_threshold = simple_volatility_baseline(\n","    returns[asset], \n","    window=7, \n","    percentile=0.95\n",")\n","\n","print(\"=\"*70)\n","print(\"BASELINE 2: Simple Rolling Volatility (95th Percentile)\")\n","print(\"=\"*70)\n","print(f\"Window: 21 days (1 month)\")\n","print(f\"Threshold: {vol_threshold:.4f} (annualized)\")\n","\n","print(f\"\\nCrisis Detection:\")\n","vol_crisis = evaluate_crisis_detection(vol_states, crisis_events)\n","for event, rate in vol_crisis.items():\n","    print(f\"  {event}: {rate}\")\n","\n","print(f\"\\nRegime Stability:\")\n","vol_metrics = calculate_regime_metrics(vol_states)\n","for metric, value in vol_metrics.items():\n","    print(f\"  {metric}: {value}\")"]},{"cell_type":"code","execution_count":14,"id":"70d5bf00","metadata":{},"outputs":[],"source":["hmm_probs = model.smoothed_marginal_probabilities[1]  # Regime 1 (crisis)\n","hmm_states = (hmm_probs > 0.5).astype(int)\n","hmm_states = pd.Series(hmm_states, index=returns.index)\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"YOUR HMM MODEL: Simple Returns + Switching Variance\")\n","print(\"=\"*70)\n","print(f\"AIC: {model.aic:.1f}\")\n","print(f\"BIC: {model.bic:.1f}\")\n","print(f\"Log-Likelihood: {model.llf:.1f}\")\n","\n","print(f\"\\nCrisis Detection:\")\n","hmm_crisis = evaluate_crisis_detection(hmm_states, crisis_events)\n","for event, rate in hmm_crisis.items():\n","    print(f\"  {event}: {rate}\")\n","\n","print(f\"\\nRegime Stability:\")\n","hmm_metrics = calculate_regime_metrics(hmm_states)\n","for metric, value in hmm_metrics.items():\n","    print(f\"  {metric}: {value}\")"]},{"cell_type":"code","execution_count":15,"id":"b0bf5f3f","metadata":{},"outputs":[],"source":["comparison =  pd.DataFrame({\n","    'Volatility MA': {**vol_crisis, **vol_metrics},\n","    'Turbulence 95th': {**turb_crisis, **turb_metrics},\n","    'HMM': {**hmm_crisis, **hmm_metrics, \n","                   'AIC': f\"{model.aic:.1f}\", \n","                   'BIC': f\"{model.bic:.1f}\"}\n","})\n","comparison"]},{"cell_type":"code","execution_count":16,"id":"046ea650","metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n","\n","# 1. SPY Returns\n","axes[0].plot(returns.index, returns.values, alpha=0.7, linewidth=0.5, color='black')\n","axes[0].axhline(0, color='red', linestyle='--', linewidth=0.8, alpha=1)\n","axes[0].set_ylabel('SPY Returns')\n","axes[0].set_title('Market Returns (SPX) - Blue Shading = HMM Crisis Detection', \n","                  fontsize=12, fontweight='bold')\n","axes[0].grid(True, alpha=0.3)\n","\n","# Add red shading for HMM crisis periods on first plot only\n","for i in range(len(hmm_states)):\n","    if hmm_states.iloc[i] == 1:\n","        # Find contiguous crisis periods\n","        if i == 0 or hmm_states.iloc[i-1] == 0:\n","            start_date = hmm_states.index[i]\n","        if i == len(hmm_states)-1 or hmm_states.iloc[i+1] == 0:\n","            end_date = hmm_states.index[i]\n","            axes[0].axvspan(start_date, end_date, alpha=0.2, color='blue', zorder=-1)\n","\n","# 2. Simple Volatility\n","axes[1].plot(rolling_vol.index, rolling_vol.values, color='purple', linewidth=1)\n","axes[1].axhline(vol_threshold, color='purple', linestyle='--', \n","                label=f'95th %ile = {vol_threshold:.4f}')\n","axes[1].set_ylabel('Volatility (Ann.)')\n","axes[1].set_title('Rolling Volatility (21-day)', fontsize=12, fontweight='bold')\n","axes[1].legend(loc='upper left')\n","axes[1].grid(True, alpha=0.3)\n","\n","# 3. Turbulence Score\n","axes[2].plot(turbulence.index, turbulence.values, color='orange', linewidth=1)\n","axes[2].axhline(threshold_95, color='orange', linestyle='--', \n","                label=f'95th %ile = {threshold_95:.1f}')\n","axes[2].set_ylabel('Turbulence')\n","axes[2].set_title('Kritzman Turbulence Index', fontsize=12, fontweight='bold')\n","axes[2].legend(loc='upper left')\n","axes[2].grid(True, alpha=0.3)\n","\n","# 4. Comparison: All three regimes overlaid\n","axes[3].fill_between(vol_states.index, 0, vol_states*0.33, \n","                     alpha=0.5, color='purple', label='Volatility')\n","axes[3].fill_between(turb_states.index, 0.34, 0.34 + turb_states*0.33, \n","                     alpha=0.5, color='orange', label='Turbulence')\n","axes[3].fill_between(hmm_states.index, 0.67, 0.67 + hmm_states*0.33, \n","                     alpha=0.5, color='blue', label='HMM')\n","axes[3].set_ylabel('Model')\n","axes[3].set_yticks([0.165, 0.5, 0.835])\n","axes[3].set_yticklabels(['Volatility', 'Turbulence', 'HMM'])\n","axes[3].set_title('Regime Detection Comparison', fontsize=12, fontweight='bold')\n","axes[3].legend(loc='upper left', ncol=3)\n","axes[3].grid(True, alpha=0.3)\n","axes[3].set_ylim(0, 1)\n","axes[3].set_xlabel('Date')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1685d9bc","metadata":{},"outputs":[],"source":["# Cell: Calculate switches with different consecutive-day filters\n","\n","def apply_consecutive_filter(states, n_days=2):\n","    \"\"\"\n","    Apply n-day consecutive filter to regime states\n","    \n","    Parameters:\n","    -----------\n","    states : pd.Series\n","        Binary regime states (0=normal, 1=crisis)\n","    n_days : int\n","        Number of consecutive days required before switching\n","    \n","    Returns:\n","    --------\n","    filtered_states : pd.Series\n","        Filtered regime states\n","    \"\"\"\n","    filtered = states.copy()\n","    \n","    # Start with first state\n","    current_regime = states.iloc[0]\n","    filtered.iloc[0] = current_regime\n","    \n","    consecutive_count = 0\n","    \n","    for i in range(1, len(states)):\n","        if states.iloc[i] != current_regime:\n","            # Different regime detected\n","            consecutive_count += 1\n","            \n","            if consecutive_count >= n_days:\n","                # Switch confirmed after n consecutive days\n","                current_regime = states.iloc[i]\n","                consecutive_count = 0\n","            \n","            # Keep previous regime until confirmed\n","            filtered.iloc[i] = current_regime\n","        else:\n","            # Same regime, reset counter\n","            consecutive_count = 0\n","            filtered.iloc[i] = current_regime\n","    \n","    return filtered\n","\n","\n","# Apply filters with different thresholds\n","print(\"=\"*70)\n","print(\"REGIME SWITCHING ANALYSIS: CONSECUTIVE-DAY FILTERS\")\n","print(\"=\"*70)\n","\n","# Original HMM states (no filter)\n","original_switches = (hmm_states.diff() != 0).sum()\n","years = len(hmm_states) / 252\n","\n","print(f\"\\nOriginal HMM (no filter):\")\n","print(f\"  Total switches: {original_switches}\")\n","print(f\"  Switches per year: {original_switches/years:.1f}\")\n","print(f\"  Crisis %: {hmm_states.mean()*100:.1f}%\")\n","\n","# Test different consecutive-day requirements\n","for n_days in [2, 3, 4]:\n","    filtered_states = apply_consecutive_filter(hmm_states, n_days=n_days)\n","    \n","    switches = (filtered_states.diff() != 0).sum()\n","    crisis_pct = filtered_states.mean() * 100\n","    \n","    print(f\"\\n{n_days}-Day Consecutive Filter:\")\n","    print(f\"  Total switches: {switches}\")\n","    print(f\"  Switches per year: {switches/years:.1f}\")\n","    print(f\"  Crisis %: {crisis_pct:.1f}%\")\n","    \n","    # Check crisis detection still works\n","    covid_mask = (filtered_states.index >= '2020-02-20') & (filtered_states.index <= '2020-04-01')\n","    if covid_mask.sum() > 0:\n","        covid_detection = filtered_states[covid_mask].mean() * 100\n","        print(f\"  COVID-19 detection: {covid_detection:.1f}%\")\n","\n","\n","# Comparison table\n","print(\"\\n\" + \"=\"*70)\n","print(\"COMPARISON TABLE\")\n","print(\"=\"*70)\n","\n","comparison_data = {\n","    'Filter': ['No filter', '2-day', '3-day', '4-day'],\n","    'Switches/Year': [],\n","    'Total Switches': [],\n","    'Crisis %': [],\n","    'COVID Detection': []\n","}\n","\n","# Original\n","comparison_data['Switches/Year'].append(original_switches/years)\n","comparison_data['Total Switches'].append(original_switches)\n","comparison_data['Crisis %'].append(hmm_states.mean()*100)\n","covid_mask = (hmm_states.index >= '2020-02-20') & (hmm_states.index <= '2020-04-01')\n","comparison_data['COVID Detection'].append(hmm_states[covid_mask].mean()*100)\n","\n","# Filtered versions\n","for n_days in [2, 3, 4]:\n","    filtered = apply_consecutive_filter(hmm_states, n_days=n_days)\n","    switches = (filtered.diff() != 0).sum()\n","    \n","    comparison_data['Switches/Year'].append(switches/years)\n","    comparison_data['Total Switches'].append(switches)\n","    comparison_data['Crisis %'].append(filtered.mean()*100)\n","    \n","    covid_mask = (filtered.index >= '2020-02-20') & (filtered.index <= '2020-04-01')\n","    comparison_data['COVID Detection'].append(filtered[covid_mask].mean()*100)\n","\n","comparison_df = pd.DataFrame(comparison_data)\n","print(comparison_df.to_string(index=False))"]},{"cell_type":"code","execution_count":26,"id":"2384dbc4","metadata":{},"outputs":[],"source":["# Apply 2-day consecutive filter to HMM states\n","filtered_states_2day = apply_consecutive_filter(hmm_states, n_days=2)\n","\n","# Align indices - use the returns/hmm_states index\n","aligned_close = close.reindex(filtered_states_2day.index)\n","\n","# Split into regime-based dataframes with filtered states\n","df_1_filtered = aligned_close[filtered_states_2day == 0]  # Low variance (normal)\n","df_2_filtered = aligned_close[filtered_states_2day == 1]  # High variance (crisis)\n","\n","# Ensure datetime index and sort\n","df_1_filtered.index = pd.to_datetime(df_1_filtered.index)\n","df_1_filtered = df_1_filtered.sort_index()\n","df_2_filtered.index = pd.to_datetime(df_2_filtered.index)\n","df_2_filtered = df_2_filtered.sort_index()\n","\n","# Plot with 2-day filter applied\n","plt.figure(figsize=(15, 10))\n","plt.scatter(df_1_filtered.index, df_1_filtered, color='blue', \n","            label=\"Low Variance Regime (2-day filter)\", s=20, alpha=0.7)\n","plt.scatter(df_2_filtered.index, df_2_filtered, color='red', \n","            label=\"High Variance Regime (2-day filter)\", s=20, alpha=0.7)\n","\n","# Add shaded region for backtesting period\n","plt.axvspan('2024-07-01', '2025-07-01', alpha=0.2, color='green', \n","            zorder=-1, label=\"Backtesting Period\")\n","\n","plt.title(\"SPY Price - HMM Regime Classification (2-Day Consecutive Filter)\", \n","          fontsize=14, fontweight='bold')\n","plt.ylabel(\"Price ($)\", fontsize=12)\n","plt.xlabel(\"Date\", fontsize=12)\n","plt.legend(loc='upper left', fontsize=10)\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary stats\n","print(\"=\"*70)\n","print(\"2-DAY FILTER APPLIED\")\n","print(\"=\"*70)\n","print(f\"Total switches: {(filtered_states_2day.diff() != 0).sum()}\")\n","print(f\"Switches per year: {(filtered_states_2day.diff() != 0).sum() / (len(filtered_states_2day)/252):.1f}\")\n","print(f\"Crisis days: {filtered_states_2day.sum()} ({filtered_states_2day.mean()*100:.1f}%)\")\n","print(f\"Normal days: {(filtered_states_2day == 0).sum()} ({(filtered_states_2day == 0).mean()*100:.1f}%)\")"]},{"cell_type":"code","execution_count":21,"id":"5ecdcada","metadata":{},"outputs":[],"source":["filtered_states_2day"]},{"cell_type":"code","execution_count":null,"id":"7f5a2d5b","metadata":{},"outputs":[],"source":["close.loc[filtered_states_2day == 0]['SPX']"]},{"cell_type":"code","execution_count":23,"id":"277f3b02","metadata":{},"outputs":[],"source":["[filtered_states_2day == 0]"]},{"cell_type":"code","execution_count":null,"id":"3b9e26f9","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Foundation-Autogluon","language":"python","name":"foundation-autogluon"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":5}